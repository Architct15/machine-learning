---
title: "Predicting Correctness of Barbell Lift Exercise"
subtitle: "Project for Practical Machine Learning Course"
author: "Simon Chan"
date: "November 18, 2015"
output: html_document
---

##Executive Summary

Wearable devices which can be used to track personal activities are very common nowadays. The most common usage is to track how much activity people do. Actually these devices can also be used to track how well people do the exercises. In this report I will take the training data of 6 participants doing the Barbell Lift exercise in 5 different manners with only one of the way being the correct way. I will perform feature selection, training with cross validation and estimation of errors. The trained model will then be used to evaluate a set of 20 readings of people doing the exercise and to predict which of the 5 ways they are doing it.  

##Data Source

The data for this report come from this source: http://groupware.les.inf.puc-rio.br/har. For reproducibility the following are the source of the training and testing data:

Training data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

Testing data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The training data set consists of 160 variables including name of participant, time of exercise, different readings for the motions tracked by the gyrometer, acclerometer and magnetometer on sensor devices mounted on the arm, forearm, belt and Barbell and in which way (A, B, C, D, E) that the individual is doing the exercise. A is the correct way and the rest are different wrong styles of doing the exercise.

##Exploratory Data Analysis

Let's first load the data and take a exploratory plot

```{r echo = TRUE, warning = FALSE}
library(caret)
set.seed(33833)
pmltrain <- read.csv("pml-training.csv"); dim(pmltrain)
pmltest <- read.csv("pml-testing.csv"); dim(pmltest)
ggplot(NULL, aes(x=colSums(is.na(pmltrain[,1:ncol(pmltrain)])))) + geom_histogram() +
    xlab("Number of NA rows (0 is better)") +
    ylab("Count of number of columns") + 
    ggtitle("Distribution of NA data")
```


##Feature Selection

From the above plot we can see there are around 60 columns with possibly all NA cells. They should provide very little predictive value. Furthermore we should also remove cells that are added on top of raw data such as time and name of individuals. We can also remove those columns with near zero variance. The following are the steps to remove the uncessary columns.

```{r, echo=TRUE}
# Step 1. Remove columns with all NA's
data.step1 <- pmltrain[, apply(pmltrain, 2, function(x) all(!is.na(x)))];dim(data.step1)

# step 2. Remove descriptive columns that is not raw data
data.step2 <- data.step1[,-(1:7)]; dim(data.step2)

# step 3. Remove columns with near zero variance
nzrcols <- nearZeroVar(data.step1)
if(length(nzrcols) > 0) {
    data.step3 <- data.step2[,-nzrcols]
} else {
    data.step3 <- data.step2
} 
dim(data.step3)

# Step 4. Remove columns with over 95% blank cells
data.step4 <- data.step3[,apply(data.step3, 2, function(x)(sum(!grepl("^\\s+$|^$",x))/nrow(data.step3) > 0.05))];dim(data.step4)

```

##Dividing Datasets for Cross Validation

I will divide the pmltrain data set into 75% training and 25% testing for cross valiation of models.  

```{r, echo=TRUE}
# Divide pmltrain dataset into 75% training and 25% testing
data.step4.sample = createDataPartition(data.step4$classe, p=0.75, list=FALSE)
data.step4.training <- data.step4[data.step4.sample,]; dim(data.step4.training)
data.step4.testing <- data.step4[-data.step4.sample,]; dim(data.step4.testing)
# Check if the testing dataset is 1/3 of training dataset and if they are evently distributed by "classe" 
summary(data.step4.testing$classe)/summary(data.step4.training$classe)
```

The above result shows that the sampling is evently distributed for "classe" variable from A to E.


##Train and Cross Validate Prediction Models

This is a classification problem and Random Forest (rf) method is usually the most accurate. I trained the model using crossvalidation with 3 folds. Then I further tested the out-of-sample accuracy using the testing set. Finallyl the accuracy is measured using a confusion matrix.

###Random Forest model Training

```{r rf, cache=TRUE, echo=TRUE}
# Train the Random Forest (rf) model
control.rf <- trainControl(method="cv", number=3, allowParallel = TRUE)
fit.rf <- train(classe ~.,data.step4.training, method="rf", trControl=control.rf, verbose=FALSE)
fit.rf
```

###Using the sliced testing data set for cross validation

```{r echo=TRUE}
# Cross validate with testing dataset
data.step4.rf.prediction <- predict(fit.rf, newdata = data.step4.testing[,-30])
```

###Estimation of out of sample errors

```{r echo=TRUE}
# Error for Random Forest prediction
cm <- confusionMatrix(data.step4.rf.prediction,data.step4.testing$classe); cm
```

The accuracy statistic as seen from the out-of-sample testing dataset prediction is `r cm$overall[1]*100`% which is very good. The error rate is `r (1 - cm$overall[1])*100`%.

##Predict with the Provided Testing Dataset

I now try to predict the 20 provided exercise readings and determine which style (A - E) they belong using the trained Random Forest model.

```{r, echo=TRUE}
# select the features from columns similar to the training data set
data.test <- pmltest[,names(data.step4[,-30])]
# predict using random forest
data.test.prediction <- predict(fit.rf, newdata = data.test)
data.test.prediction
```

The above results are the predicted styles (A-E) for the provided testing dataset.

###Output of the prediction results
```{r echo=TRUE}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(data.test.prediction)
```
